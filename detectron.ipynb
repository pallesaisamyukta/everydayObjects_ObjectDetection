{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7836435,"sourceType":"datasetVersion","datasetId":4593484},{"sourceId":7870418,"sourceType":"datasetVersion","datasetId":4618100}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import shutil\nimport os\nimport yaml\nimport json\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-24T16:46:32.411954Z","iopub.execute_input":"2024-03-24T16:46:32.412323Z","iopub.status.idle":"2024-03-24T16:46:33.703239Z","shell.execute_reply.started":"2024-03-24T16:46:32.412293Z","shell.execute_reply":"2024-03-24T16:46:33.702417Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Define your source directories\nsource_images_dir = '/kaggle/input/customdataset/ObjectDetection/images/train'\nyolo_annotations_dir = '/kaggle/input/customdataset/ObjectDetection/labels/train'\ncoco_annotations_path = '/kaggle/input/coco-annotations/instances_default.json'\n\n# Define the output base directories\nbase_dir = '/kaggle/working'\nimages_dir = os.path.join(base_dir, 'images')\nlabels_dir = os.path.join(base_dir, 'labels')\ncoco_dir = os.path.join(base_dir, 'coco_annotations')\n\n# Make sure the base directories exist\nos.makedirs(images_dir, exist_ok=True)\nos.makedirs(labels_dir, exist_ok=True)\nos.makedirs(coco_dir, exist_ok=True)\n\n# Prepare subdirectories for train and val in each base directory\nfor sub_dir in ['train', 'val']:\n    os.makedirs(os.path.join(images_dir, sub_dir), exist_ok=True)\n    os.makedirs(os.path.join(labels_dir, sub_dir), exist_ok=True)\n\n# List and split image files\nimage_files = [f for f in os.listdir(source_images_dir) if f.endswith('.jpg')]\ntrain_files, val_files = train_test_split(image_files, test_size=0.3, random_state=42)\n\n# Load COCO annotations\nwith open(coco_annotations_path) as f:\n    coco_annotations = json.load(f)\n\ndef filter_coco_annotations(files, annotations):\n    filtered = {'images': [], 'annotations': [], 'categories': annotations['categories']}\n    image_ids = {os.path.splitext(f)[0] for f in files}\n    filtered['images'] = [image for image in annotations['images'] if image['file_name'].replace('.jpg', '') in image_ids]\n    filtered_image_ids = {image['id'] for image in filtered['images']}\n    filtered['annotations'] = [ann for ann in annotations['annotations'] if ann['image_id'] in filtered_image_ids]\n    return filtered\n\ndef organize_dataset(files, source_images_dir, yolo_dir, images_dest_dir, labels_dest_dir):\n    for filename in files:\n        # Copy image\n        shutil.copy(os.path.join(source_images_dir, filename), os.path.join(images_dest_dir, filename))\n        \n        # Copy YOLO annotation\n        yolo_annotation_filename = filename.replace('.jpg', '.txt')\n        shutil.copy(os.path.join(yolo_dir, yolo_annotation_filename), os.path.join(labels_dest_dir, yolo_annotation_filename))\n\n# Filter COCO annotations for train and val sets\ntrain_annotations_coco = filter_coco_annotations(train_files, coco_annotations)\nval_annotations_coco = filter_coco_annotations(val_files, coco_annotations)\n\n# Organize train and validation datasets\norganize_dataset(train_files, source_images_dir, yolo_annotations_dir, os.path.join(images_dir, 'train'), os.path.join(labels_dir, 'train'))\norganize_dataset(val_files, source_images_dir, yolo_annotations_dir, os.path.join(images_dir, 'val'), os.path.join(labels_dir, 'val'))\n\n# Save filtered COCO annotations\nwith open(os.path.join(coco_dir, 'train.json'), 'w') as f:\n    json.dump(train_annotations_coco, f)\nwith open(os.path.join(coco_dir, 'val.json'), 'w') as f:\n    json.dump(val_annotations_coco, f)\n\nprint(\"Dataset organized with separate directories for images, YOLO labels, and COCO annotations.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:46:33.705196Z","iopub.execute_input":"2024-03-24T16:46:33.705577Z","iopub.status.idle":"2024-03-24T16:46:36.212465Z","shell.execute_reply.started":"2024-03-24T16:46:33.705552Z","shell.execute_reply":"2024-03-24T16:46:36.211497Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Dataset organized with separate directories for images, YOLO labels, and COCO annotations.\n","output_type":"stream"}]},{"cell_type":"code","source":"# The path to the original YAML file\ninput_yaml_path = '/kaggle/input/customdataset/ObjectDetection/google_colab_files.yaml'\n# The path where the modified YAML file will be saved\noutput_yaml_path = '/kaggle/working/google_colab_files.yaml'\n\n# Load the original YAML file\nwith open(input_yaml_path, 'r') as file:\n    data = yaml.safe_load(file)\n\n# Modify the dataset configuration as needed\ndata['path'] = '/kaggle/working'\ndata['val'] = 'images/val'  # Assuming you want to point to the validation dataset\n\n# Save the modified configuration to a new YAML file\nwith open(output_yaml_path, 'w') as file:\n    yaml.dump(data, file, sort_keys=False)  # sort_keys=False to preserve the order of keys","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:46:36.214696Z","iopub.execute_input":"2024-03-24T16:46:36.215026Z","iopub.status.idle":"2024-03-24T16:46:36.224956Z","shell.execute_reply.started":"2024-03-24T16:46:36.214991Z","shell.execute_reply":"2024-03-24T16:46:36.224195Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools\n!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:46:36.226742Z","iopub.execute_input":"2024-03-24T16:46:36.227038Z","iopub.status.idle":"2024-03-24T16:48:55.870067Z","shell.execute_reply.started":"2024-03-24T16:46:36.227006Z","shell.execute_reply":"2024-03-24T16:48:55.868824Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pycocotools\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\nCollecting git+https://github.com/facebookresearch/detectron2.git\n  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-7sps9zdo\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-7sps9zdo\n  Resolved https://github.com/facebookresearch/detectron2.git to commit eb96ee1d4752ff5896f623f738641fba9c755237\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (9.5.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (3.7.5)\nRequirement already satisfied: pycocotools>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (2.0.7)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (2.4.0)\nCollecting yacs>=0.1.8 (from detectron2==0.6)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (0.9.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (2.2.1)\nRequirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (4.66.1)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (2.15.1)\nCollecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\nCollecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting hydra-core>=1.1 (from detectron2==0.6)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting black (from detectron2==0.6)\n  Downloading black-24.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (8.1.7)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (1.0.0)\nCollecting packaging (from detectron2==0.6)\n  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\nCollecting pathspec>=0.9.0 (from black->detectron2==0.6)\n  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (4.2.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (2.0.1)\nRequirement already satisfied: typing-extensions>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (4.9.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (3.5.2)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\nDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading black-24.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading packaging-24.0-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n  Building wheel for detectron2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=1261224 sha256=2d76c661fcc4e52bccc3a1244c813c0a0a4b1e3adadfe794b4248a8a00577866\n  Stored in directory: /tmp/pip-ephem-wheel-cache-rtsbbsl2/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=7823bc175b183f75399d37943ecb3aeda4513f81f3c2f36137f544e8b7fe9b39\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=d2985a93c45378368024044e22a75da1d587a6c860fbd6d2203ad7604e9fceb8\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built detectron2 fvcore antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, packaging, omegaconf, iopath, hydra-core, black, fvcore, detectron2\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.0 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 black-24.3.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 omegaconf-2.3.0 packaging-24.0 pathspec-0.12.1 portalocker-2.8.2 yacs-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"from detectron2.data.datasets import register_coco_instances\n\nregister_coco_instances(\"coco_dataset_train\", {}, \"/kaggle/working/coco_annotations/train.json\", \"/kaggle/working/images/train\")\nregister_coco_instances(\"coco_dataset_val\", {}, \"/kaggle/working/coco_annotations/val.json\", \"/kaggle/working/images/val\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:48:55.872497Z","iopub.execute_input":"2024-03-24T16:48:55.872879Z","iopub.status.idle":"2024-03-24T16:49:00.755534Z","shell.execute_reply.started":"2024-03-24T16:48:55.872840Z","shell.execute_reply":"2024-03-24T16:49:00.754780Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from detectron2.config import get_cfg\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultTrainer\n\ncfg = get_cfg()\n# Use model_zoo to get the path for the config file\nconfig_path = model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.merge_from_file(config_path)\n\ncfg.DATASETS.TRAIN = (\"coco_dataset_train\",)\ncfg.DATASETS.TEST = (\"coco_dataset_val\",)\ncfg.DATALOADER.NUM_WORKERS = 2\n\n# Let training initialize from model zoo\ncfg.MODEL.WEIGHTS = \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"  # Initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.0025\ncfg.SOLVER.MAX_ITER = 1000   # Adjust according to your dataset size\ncfg.OUTPUT_DIR = '/kaggle/working/results'\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:49:00.756538Z","iopub.execute_input":"2024-03-24T16:49:00.756900Z","iopub.status.idle":"2024-03-24T16:58:09.888559Z","shell.execute_reply.started":"2024-03-24T16:49:00.756876Z","shell.execute_reply":"2024-03-24T16:58:09.887723Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"R-50.pkl: 102MB [00:03, 26.6MB/s]                              \n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n2024-03-24 16:49:23.266426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-24 16:49:23.266530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-24 16:49:23.373815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from detectron2.data import build_detection_test_loader\nfrom detectron2.evaluation import inference_on_dataset\nfrom detectron2.evaluation import COCOEvaluator\nimport time\nimport torch \n# Ensure the model is in evaluation mode\ntrainer.model.eval()\n\n# Load the validation dataset\nval_loader = build_detection_test_loader(cfg, \"coco_dataset_val\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:00:47.009802Z","iopub.execute_input":"2024-03-24T17:00:47.010513Z","iopub.status.idle":"2024-03-24T17:00:47.023945Z","shell.execute_reply.started":"2024-03-24T17:00:47.010479Z","shell.execute_reply":"2024-03-24T17:00:47.022987Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"total_time = 0\ntotal_images = 0\n\n# Perform inference and measure time per image\nfor idx, inputs in enumerate(val_loader):\n    start_time = time.time()\n    with torch.no_grad():\n        predictions = trainer.model(inputs)\n    end_time = time.time()\n    total_time += (end_time - start_time)\n    total_images += len(inputs)\n\n# Calculate average inference time per image\naverage_time_per_image = total_time / total_images\nprint(f\"Average inference time per image: {average_time_per_image:.4f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:00:48.042068Z","iopub.execute_input":"2024-03-24T17:00:48.042789Z","iopub.status.idle":"2024-03-24T17:00:58.054422Z","shell.execute_reply.started":"2024-03-24T17:00:48.042760Z","shell.execute_reply":"2024-03-24T17:00:58.053224Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Average inference time per image: 0.0732 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a COCOEvaluator instance for the validation dataset\nevaluator = COCOEvaluator(\"coco_dataset_val\", cfg, False, output_dir=\"./output/\")\n\ntotal_time = 0\ntotal_images = 0\n\n# Run inference on the dataset and evaluate the results using the evaluator\nprint(\"Evaluating model on COCO validation dataset...\")\nevaluation_results = inference_on_dataset(trainer.model, val_loader, evaluator)\n\n# Print evaluation results\nprint(evaluation_results)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:00:58.056233Z","iopub.execute_input":"2024-03-24T17:00:58.056581Z","iopub.status.idle":"2024-03-24T17:01:08.127497Z","shell.execute_reply.started":"2024-03-24T17:00:58.056549Z","shell.execute_reply":"2024-03-24T17:01:08.126387Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Evaluating model on COCO validation dataset...\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.460\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.169\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.202\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.180\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.296\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.302\nOrderedDict([('bbox', {'AP': 20.2398630207728, 'AP50': 45.995585209382014, 'AP75': 16.85093897816451, 'APs': nan, 'APm': nan, 'APl': 20.2398630207728, 'AP-drink': 11.879875487548757, 'AP-utensil': 8.789059504406039, 'AP-laptop': 40.0506540703636, 'AP-no_class': nan})])\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\n# Path to the COCO annotations file\nannotations_file = '/kaggle/input/coco-annotations/instances_default.json'\n\n# Load the COCO annotations\nwith open(annotations_file, 'r') as f:\n    coco_data = json.load(f)\n\n# Initialize a dictionary to hold the count of each category\ncategory_counts = {}\n\n# Iterate over all annotations to count the instances per category\nfor annotation in coco_data['annotations']:\n    category_id = annotation['category_id']\n    if category_id in category_counts:\n        category_counts[category_id] += 1\n    else:\n        category_counts[category_id] = 1\n\n# Translate category IDs to category names (for readability)\ncategory_names = {category['id']: category['name'] for category in coco_data['categories']}\ncategory_counts_named = {category_names[cat_id]: count for cat_id, count in category_counts.items()}\n\n# Print the count of instances for each category\nfor category, count in category_counts_named.items():\n    print(f'Category: {category}, Instances: {count}')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-25T03:08:09.554111Z","iopub.execute_input":"2024-03-25T03:08:09.554509Z","iopub.status.idle":"2024-03-25T03:08:09.587281Z","shell.execute_reply.started":"2024-03-25T03:08:09.554472Z","shell.execute_reply":"2024-03-25T03:08:09.586161Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Category: drink, Instances: 255\nCategory: utensil, Instances: 144\nCategory: laptop, Instances: 138\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}